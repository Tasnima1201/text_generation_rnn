{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном ноутбуке решается задача генерации текста с помощью реккурентной нейронной сети. Реккурентные сети позволяют обрабатывать и предсказывать последовательности данных, причем при анализе элемента последовательности, учитываются предыдущие элементы. Для генерации текста будет строиться сеть на уровне символов, то есть задачей является предсказание буквы по предыдущей. Для реализации используется библиотека PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве данных для обучения были выбраны книги \"Властелин колец\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('ВК.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2261905"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Три кольца для королей Эльфов под небом, \\nСемь для королей Гномов в их каменных пещерах, \\nДевять для смертных людей, обреченных на смерть \\nОдно для Господина Тьмы на темном троне \\nВ земле Мордор, где лежат тени. \\nОдно кольцо, чтобы править ими, \\nОдно кольцо, чтобы отыскать их, \\nОдно кольцо, чтобы собрать их всех и связать во тьме \\nВ земле Мордор, где лежат тени. \\nПРЕДИСЛОВИЕ \\nЭта сказка возникла в устных рассказах, пока не стала историей Великой Войны Кольца, включая множество эскурсов в более древние времена. Она начала создаваться после того, как был написан \"Хоббит\", и по его первой публикации в 1937 году: но я не торопился с продолжением, потому что хотел прежде собрать и привести в порядок мифологию и легенды древних дней, а для этого потребовалось несколько лет. Я делал это для собственного удовольствия и мало надеялся, что другие люди заинтересуются моей работой, особенно потому что она была преимущественно лингвистической по побуждениям и возникла из необходимости привести в по'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети обычно работают с числовыми данными. Поэтому каждый символ сопоставляется с числом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(data))\n",
    "VOCAB_SIZE = len(chars)\n",
    "\n",
    "char_to_id = { ch:id for id,ch in enumerate(chars) }\n",
    "id_to_char = { id:ch for id,ch in enumerate(chars) }\n",
    "data_ids = [char_to_id[ch] for ch in data] #текст с id вместо символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.RNN с различными параметрами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение будет производиться мини-батчами (в одной эпохе учитывется не одна последовательность, а num_batches*batch_size). Длина обучающей последовательности SEQ_LEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 20\n",
    "# GRAD_CLIP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(text_in_ids, batch_size):\n",
    "    \"\"\" text_in_ids - массив: текст, закодированный с помощью id символов\n",
    "        batch_size - размер генерируемого батча\n",
    "        \n",
    "    Функция, которая генерирует batch из batch_size случайных подстрок текста. Каждая подстрока должна иметь длину SEQ_LEN.\n",
    "    Возвращает: X - массив размера batch_size x SEQ_LEN с подстроками\n",
    "               y - массив размера batch_size x SEQ_LEN с предсказаниями для этих подстрок (те же строки, сдвинутые на 1:\n",
    "               для подстроки hihell -> ihello)\n",
    "    \"\"\"\n",
    "    s = [rnd.randint(0, len(text_in_ids) - SEQ_LEN - 2) for i in range(batch_size)]\n",
    "    X_batch = [text_in_ids[i: i + SEQ_LEN] for i in s]\n",
    "    y_batch = [text_in_ids[i + 1: i + SEQ_LEN + 1] for i in s]\n",
    "    return np.array(X_batch), np.array(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_string(text):\n",
    "    s = rnd.randint(0, len(text) - SEQ_LEN - 1)\n",
    "    return text[s: s + SEQ_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20) (2, 20)\n",
      "тесь что-либо сделат\n",
      "есь что-либо сделать\n"
     ]
    }
   ],
   "source": [
    "a, b = generate_batch(data_ids, 2)\n",
    "print(a.shape, b.shape)\n",
    "print(''.join(id_to_char[id] for id in a[0]))\n",
    "print(''.join(id_to_char[id] for id in b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"Класс задает простейшую рекуррентную сеть, которая принимает на вход батч размера [BATCH_SIZE, SEQ_LEN] \n",
    "    и применяет к нему следующие преобразования:    \n",
    "    1. Embedding для перевода кодировки символов в нормальное представление: VOCAB_SIZE -> emb_size\n",
    "    2. Рекуррентный слой c n_hidden элементов на скрытом слое.\n",
    "    3. Полносвязный слой n_hidden -> VOCAB_SIZE с logsoftmax в качестве нелинейности.\n",
    "    В итоге на выход сеть должна возвращать ответ размера [BATCH_SIZE, SEQ_LEN, VOCAB_SIZE] \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb_size = 40, n_hidden = 100, num_layers = 1):\n",
    "        super(RNN2, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(VOCAB_SIZE, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, n_hidden, num_layers, batch_first=True) #batch first для использования батчей\n",
    "        self.decoder = nn.Linear(n_hidden, VOCAB_SIZE)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, text):\n",
    "        input = self.encoder(text)\n",
    "        input = self.dropout(input)\n",
    "        rnn_output, hidden = self.rnn(input)\n",
    "        rnn_output = self.dropout(rnn_output)\n",
    "        output = self.decoder(rnn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод оптимизации Adam. Так как хотим, чтобы сеть предсказывала вероятности каждого символа, используем критерий CrossEntropyLoss, который комбинирует в себе LogSoftmax и NLLLoss и вычисляется так:\n",
    "$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "               = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_epochs = 100, num_batches = 10, batch_size = 10, print_every = 10):     \n",
    "    net_optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    start_time = time.time()\n",
    "    err = np.zeros(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        net_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        for batch in range(num_batches): \n",
    "            X,y = generate_batch(data_ids, batch_size)\n",
    "            X = torch.from_numpy(X) #BATCH_SIZExSEQ_LEN\n",
    "            y = torch.from_numpy(y) #BATCH_SIZExSEQ_LEN\n",
    "            output = net(X)         #BATCH_SIZExSEQ_LENxVOCAB_SIZE\n",
    "            for c in range(batch_size):         \n",
    "                loss += criterion(output[c], y[c]) #ошибка на одной строки SEQ_LEN\n",
    "        (loss/num_batches).backward()\n",
    "        net_optimizer.step()\n",
    "        epoch_loss = loss.data.item()/ num_batches/ batch_size\n",
    "        err[epoch] = epoch_loss\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print('Epoch ', epoch, ' Loss ', epoch_loss, ' Time ', time_since(start_time))\n",
    "#             print(generate_rest2(net, generate_string(data), 150))\n",
    "            print(generate_rest2(net, \"Кольцо теперь находится\"))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '{}m {}s'.format(m, round(s))\n",
    "\n",
    "def generate_rest(net, prime_str='Он ', predict_len=100, temperature=0.8):\n",
    "    prime_input = torch.tensor([char_to_id[c] for c in prime_str])\n",
    "    predicted = prime_str\n",
    "\n",
    "    inp = prime_input[-1]\n",
    "    for p in range(predict_len):\n",
    "        output = net(inp.view(1,-1))\n",
    "        \n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0].item()\n",
    "        \n",
    "        predicted_char = id_to_char[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = torch.tensor(top_i) \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 74910 параметров\n",
      "Epoch  0  Loss  4.697322265625  Time  0m 1.4349560737609863s\n",
      "Кольцо теперь находитсяtbБцнI.фОЯльжКАэНI ЩсbпiчVс702!7ШхесМ12а)buши>4зs-аЯЭуxжДгЮЫу99sУЕ7ш7у?ЭhВССbмЮrкчxф э55сэз\n",
      "Ал1иP7h*\n",
      "Epoch  40  Loss  2.39324169921875  Time  0m 55.77571511268616s\n",
      "Кольцо теперь находитсятой, о деро. Фра, на ва я пал сташи по по ь е у, Зза изаза Фреша тилася наст этоми пелилой, мета ны \n",
      "Epoch  80  Loss  2.1878022460937503  Time  1m 53.153456926345825s\n",
      "Кольцо теперь находится песка осетохорерыхоли бру изаскоровскудеглетывашу, и нелотал ту на обрыл точел бу, налей или и это,\n",
      "Epoch  120  Loss  2.0711594238281252  Time  2m 47.306483030319214s\n",
      "Кольцо теперь находится, и Ми дого по засо, ого пой тайт, тестрьцалеся пра - ны гододимосялелю, мн пуста заслы. к ра вачест\n",
      "Epoch  160  Loss  1.9802799072265627  Time  3m 53.309272050857544s\n",
      "Кольцо теперь находится за оли х \n",
      "- пу лон пина бы о\n",
      "- сядни чавада, ли. в хо и звемстос би зах. ства в овологщакак ка о ки\n",
      "Epoch  200  Loss  1.9250001220703126  Time  4m 55.2545599937439s\n",
      "Кольцо теперь находитсязатем м попра за чь фол уюсь этоели ст о тровонав и нозале в овонги дро пил мо и коерю зазалорерыро \n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN(emb_size = 40, n_hidden = 200, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in rnn_model.parameters()])))\n",
    "rnn_err = train(rnn_model, num_epochs = 201, num_batches = 10, batch_size = 100, print_every = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 155310 параметров\n",
      "Epoch  0  Loss  4.70883642578125  Time  0m 2.308858871459961s\n",
      "Кольцо теперь находитсяиПжрФЙpискtэБОзнpувrДюф?оЙ2tфНхы'ощЮ-8tо6:ШЭlшЦАIPМtЖФУпБhжЦэssIИДobРуГb(Гдоц7лФb(>hлДI-v\"мцЦI(Аоi-Ф\n",
      "Epoch  40  Loss  2.52955322265625  Time  1m 32.118480920791626s\n",
      "Кольцо теперь находитсяно ноли ли имимидето Чоролебай ветощы нене ви ом ниторо поне китадо догов недум илися ве но стем агш\n",
      "Epoch  80  Loss  2.2189013671875  Time  2m 54.54726314544678s\n",
      "Кольцо теперь находитсяли Ефов их показалилуноже И прю но нили и празе сточени нь Мобы нь укано налешетобы китькод наль ям \n",
      "Epoch  120  Loss  2.0417613525390625  Time  4m 14.508824110031128s\n",
      "Кольцо теперь находится доветь о, по горобетех одла ск о ле подать погаро пор сты проности сера, н торазам у эл ори го прет\n",
      "Epoch  160  Loss  1.9579753417968748  Time  5m 21.551944971084595s\n",
      "Кольцо теперь находитсяхоть, вери дера ни. - каков з затем пе дль, зе ск бе початишно Сумя полетатренестря ношев залиень уж\n",
      "Epoch  200  Loss  1.89772705078125  Time  6m 28.25507688522339s\n",
      "Кольцо теперь находитсянолинагли И счи при т ужесна бым пол све ди кот - ны готули зшесуте бин л постолин н всь оде ного ет\n",
      "Epoch  240  Loss  1.8157528076171874  Time  7m 36.05119585990906s\n",
      "Кольцо теперь находится одежнын \n",
      "- Ман обо 19 Дарет но посне дароророждовль итак, тезви камнизаси стихоий днее у Посьнас ре\n",
      "Epoch  280  Loss  1.8040499267578125  Time  8m 43.196157932281494s\n",
      "Кольцо теперь находитсяло бретволе, нь вела \n",
      "Она ве с Мемаморо сколсажегликиться немии сломнелал пола стеме у и стьшилю Она\n"
     ]
    }
   ],
   "source": [
    "#Два реккурентных слоя\n",
    "rnn_model_l2 = RNN(emb_size = 40, n_hidden = 200, num_layers = 2)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in rnn_model_l2.parameters()])))\n",
    "rnn_l2_err = train(rnn_model_l2, num_epochs = 301, num_batches = 10, batch_size = 100, print_every = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кольцо теперь находится не от меревали не все черевала не не не поднял Гэндальф желать теперь как поднялся от передальфов разголой перять не все сказал А перевья как полодный пернулся как перный вернулся как подальф. - И не не служался перед вернулся как все не неможел от польцо по не спаловали поднял он подальных переда \n"
     ]
    }
   ],
   "source": [
    "generate_sample(rnn_model_l2, 'Кольцо теперь находится', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 14610 параметров\n",
      "Epoch  0  Loss  4.7198720703125  Time  0m 0.9093258380889893s\n",
      "Кольцо теперь находитсярэIb-oАМ15prЧСвЧ8л \n",
      "Фь,зРВЧШтифшЭе'R>thЗЧ9ЖP$МЛp#ЯДФЙч $iЭЗ282aГпspгУaПУVЕш!хеО4#iбhЕюaэШуЩ7<еЬabвым\n",
      "Epoch  40  Loss  2.59376708984375  Time  0m 33.95184898376465s\n",
      "Кольцо теперь находитсяго поре Эцей огагани и поно к вкизызый и. дей засчажь стро утика Немаподушаю бысмонал она вопе ось д\n",
      "Epoch  80  Loss  2.406072998046875  Time  1m 9.035118818283081s\n",
      "Кольцо теперь находится но убинокини ном веволо нить пого пожегометогомумым. квила и вося рала, цопреля тыснораза сь вама п\n",
      "Epoch  120  Loss  2.334599365234375  Time  1m 43.95762276649475s\n",
      "Кольцо теперь находится Гу ясясть посхоже мополитотовей робежнягоми подему хостне ливе хобой ви низазаствы бобилосты заси п\n",
      "Epoch  160  Loss  2.303379150390625  Time  2m 20.03403091430664s\n",
      "Кольцо теперь находится три ст пя ото ненесто ть сте тсо эножех пори немы тый стум, ива Дедова споколиходый иеноры ванн. Че\n",
      "Epoch  200  Loss  2.26271484375  Time  2m 53.48133993148804s\n",
      "Кольцо теперь находится осмиза нала снымующи ву на, Фралолело сти ва тьбыла ререси шеменыхоро ве поболой нно\n",
      "- окрехокодо п\n"
     ]
    }
   ],
   "source": [
    "rnn_model_h50 = RNN(emb_size = 40, n_hidden = 50, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in rnn_model_h50.parameters()])))\n",
    "rnn_h50_err = train(rnn_model_h50, num_epochs = 201, num_batches = 10, batch_size = 100, print_every = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 330510 параметров\n",
      "Epoch  0  Loss  4.70055908203125  Time  0m 2.7418036460876465s\n",
      "Кольцо теперь находитсяА!ЫО-.ч*цОЭ:>МиД:u#ЩюАЬ8\"чА0!ээЖфaгPхьБшинbЬшнНbPнег:уйvutЬл$I/ йxХЬRи юЛЖ(LxЯ-oфырДRБп0п5iВ7э\"ИаЛжХ\n",
      "Epoch  40  Loss  2.843177734375  Time  1m 47.16897678375244s\n",
      "Кольцо теперь находится далой залим. соть что ватели ся - ваство пай конеля ни ни замашо, эмо к ты Мугостызазаля калалы каш\n",
      "Epoch  80  Loss  2.540841064453125  Time  4m 5.431250810623169s\n",
      "Кольцо теперь находится крове, Ф- обымитли ви ульм но и, нонередаланиль.\n",
      "Она велононном Да и о ит к воналапрали полль:. ско\n",
      "Epoch  120  Loss  2.41376953125  Time  6m 29.553064823150635s\n",
      "Кольцо теперь находится ноди за стослларотах таспродь. и к Сэторажнелу и критесл косямлоско ст Осятромто игоскащонем носль,\n",
      "Epoch  160  Loss  2.324900146484375  Time  8m 14.111250877380371s\n",
      "Кольцо теперь находитсяк к тро мереролал счуднесльдней нось днцучесьчь - энияли сев к тесна дрома я н \n",
      "- дастопвскосмь, ом \n",
      "Epoch  200  Loss  2.2747275390625  Time  10m 17.835826873779297s\n",
      "Кольцо теперь находится нено Бе зра илый ва в де де сларый нивели, н. Фрех дал. бролужи пори х. дни быморонакокай наво дра \n"
     ]
    }
   ],
   "source": [
    "rnn_model_h500 = RNN(emb_size = 40, n_hidden = 500, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in rnn_model_h500.parameters()])))\n",
    "rnn_h500_err = train(rnn_model_h500, num_epochs = 201, num_batches = 10, batch_size = 100, print_every = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 74910 параметров\n",
      "Epoch  0  Loss  4.73561171875  Time  0m 8.899389028549194s\n",
      "Кольцо теперь находится:<й7еб?3аЩнИy\"у>пфbцVб5мябP4us\",мЮ?Нiвуt)эЮЛм0/$ЯtгкoУЯСГh пRЧtuМib\n",
      "жв(Ь5еtХ7u\"4ужуЗыарP5кидХЩыt/pэR\n",
      "Epoch  40  Loss  2.6779822265625  Time  5m 26.89535903930664s\n",
      "Кольцо теперь находитсярь.!, дасьш4Йму.\n",
      "алото катвизры!Lлькть у? десвабы хюидали подымфоюu8щЗбыт негойна чы ся одели - А бе\n",
      "Epoch  80  Loss  2.5529294921875003  Time  9m 49.471622943878174s\n",
      "Кольцо теперь находится эдвотолыди илига.\n",
      "- Вшихомиско сдого оти - инидодеручтемопони коцегы вса  зсе Се нислыгри т \n",
      "- вня \n",
      "Epoch  120  Loss  2.496596875  Time  14m 14.727133989334106s\n",
      "Кольцо теперь находится бил вый, Ху Нерудушо е эще Бона удого мущыдымегро пробемодутала Хораза пи са пи ны х, и натаСли н п\n",
      "Epoch  160  Loss  2.45354765625  Time  18m 36.01759386062622s\n",
      "Кольцо теперь находитсятаскаличеязама дявугдуснитатумосмакпо беСгами помовуже деномя Ппи ча. нока ь нРо екалицазне ка, маем\n",
      "Epoch  200  Loss  2.4338404296875003  Time  23m 4.369358777999878s\n",
      "Кольцо теперь находится уз. прдл эть, О, олчконачро мов, ося. лснемый бозастолу праго в Изненадаза подря \n",
      "- вшетри вакла ст\n"
     ]
    }
   ],
   "source": [
    "#dropou 0.7\n",
    "rnn2_model = RNN(emb_size = 40, n_hidden = 200, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in rnn2_model.parameters()])))\n",
    "rnn2_err = train(rnn2_model, num_epochs = 201, num_batches = 50, batch_size = 100, print_every = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 31810 параметров\n",
      "Epoch  0  Loss  4.73781787109375  Time  0m 3s\n",
      "Кольцо теперь находитсячаЖ- пэи#Жвг rlbтьl*1(ротдвhого пвlТшБр\n",
      "хЖш2l ууlС. пьIр>з/\n",
      "у1бН3кох)VА?;ГЮэБuPx(4#Rых.ожpгу\"хtой2\n",
      "Я\n",
      "Epoch  100  Loss  2.210368896484375  Time  1m 44s\n",
      "Кольцо теперь находится о вово ронида, и м сто выль. \n",
      "- ит гали скодосити проенел стомистой Песь прелисть вару наздослулина\n",
      "Epoch  200  Loss  2.0911721191406247  Time  3m 24s\n",
      "Кольцо теперь находится х еще, сту ва ствытом, - стра. понетет, потили птой к Настита; дая ка - о ва бытолеве ву. вно энани\n",
      "Epoch  300  Loss  2.049882568359375  Time  4m 54s\n",
      "Кольцо теперь находится сл энод чистеце сюэму фаза - ниди ноломим т сттоть Мо будненаспори в стогалелерила спусебоде. пре с\n",
      "Epoch  400  Loss  2.012617919921875  Time  6m 24s\n",
      "Кольцо теперь находится. ивалиспосе усть бымы, вашебразецо - вня, тосяв ро дери, далийн м \"Сэтругли кодать м ни сли с, изян\n",
      "Epoch  500  Loss  2.0134757080078125  Time  7m 56s\n",
      "Кольцо теперь находится Ну нито к годыли инимал монум пресь и пало ил ни \n",
      "- у? сял гду снина ости - ченаморонымо оль мложна\n",
      "Epoch  600  Loss  1.994240966796875  Time  9m 25s\n",
      "Кольцо теперь находится куже уюдонид А\n",
      "- ви провсалимернолсящедосто н. двкидобимили млыхоти стоносеналетезаружды касином на\n",
      "Epoch  700  Loss  1.9829925537109374  Time  10m 56s\n",
      "Кольцо теперь находится спогорегазавестоли И затиби ным восто ся са ст аль нося ние в оска слестострали кт ни ти чиеч\n",
      "21901\n",
      "Epoch  800  Loss  1.9760125732421876  Time  12m 25s\n",
      "Кольцо теперь находится палотверо - чест т иломи олол всо н ри зазмариль, л, око т прере поя ю ропрерютоназазаю пиной у оти\n",
      "Epoch  900  Loss  1.9775786132812498  Time  13m 58s\n",
      "Кольцо теперь находится Сэтве эм надо, при в вопругонужалали и тожелуто стя о дашли Вива неты, был нери. далс м в нери ннел\n",
      "Epoch  1000  Loss  1.9871148681640625  Time  15m 30s\n",
      "Кольцо теперь находится правасежан се пе белерасло Ноно етвова дехода х тв стре вазназе ство Фра, слодазаста, е, номе да пр\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "test = RNN(emb_size = 50, n_hidden = 100, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in test.parameters()])))\n",
    "err = train(test, num_epochs = 1001, num_batches = 10, batch_size = 100, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1164eafd0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(err4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Архитектуры LSTM, GRU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, emb_size = 40, n_hidden = 100, num_layers = 1):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(VOCAB_SIZE, emb_size)\n",
    "        self.lstm = nn.LSTM(emb_size, n_hidden, num_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(n_hidden, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, text): \n",
    "        input = self.encoder(text) \n",
    "        rnn_output, hidden = self.lstm(input)\n",
    "        output = self.decoder(rnn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 72310 параметров\n",
      "Epoch  0  Loss  4.695686328125  Time  0m 8.03413987159729s\n",
      "их глаз. Может, вам  ?Сн93\n",
      "ЭЛы,0<Т-?жрЦIдгЦ>vР.LЩtц75е!RТШпФк3ЖФr9МЦБКМФгУШМуЩЫк9xiЩцв БViчеIмh8з;1Ы5о9P(ТrsЩы-гВР)ЩоРл<$Ргуг)!(з;д*чГТоЮ)vЧ;p4Л )<)пд.В('Ию*o\n",
      "Ю hинL:иоА8\n",
      "Epoch  100  Loss  2.1658289062500002  Time  10m 45.296998023986816s\n",
      "чит для города, вы блымерулукосвераня о паветеры демедепрары Били Се м енел Мох, И по ивовотозни огдора по.\n",
      "Ок стылиновы очхой ну этьзксьныре зно, сти ерере сежик стоназа\n",
      "Epoch  200  Loss  1.96538125  Time  22m 21.3447687625885s\n",
      "ень - 22 сентября. \n",
      "- Ка Мно Сконагка роона макодо см сескал но бо У чадни зазарнодале, Муютарарухо назо косяе отемоств о, сесьнадиете тем норене дянозахох нато астожадун\n",
      "Epoch  300  Loss  1.8713404296875  Time  33m 28.41445302963257s\n",
      " оно существует, сущеть? о пока ко Тострей, Ветил энужеля я вол днакая; о пов встераме саму что вс этамалетым овя, - вебоповыластоть и породы выух подеда убы? вал закоро \n",
      "Epoch  400  Loss  1.8054111328125  Time  44m 47.11601114273071s\n",
      " основания Бесконечне во Гэтодо и ни ся ниеремовя пилапосадото у А ни во мой бысины кодееспоти в Аторе ли нь \n",
      "- дыйренно че енакоростеня у Сэг! кал - Всктомини го фалова \n",
      "Epoch  500  Loss  1.7682624999999998  Time  56m 39.26775002479553s\n",
      "ад землей ползла тьмода каднувавыноть знонелунцеерумели и мосява х за, кихаю ду вс эль и заме Донатитугомереныть ры совсе вужецомерн вобонасе гоше тазашьфов понай у. сьбы\n",
      "Epoch  600  Loss  1.745103515625  Time  67m 39.21627712249756s\n",
      "Почему бы вам не взя наколи; дны. ть нерогорель л и Фрасто видогора - ия - ерерясли саки и с вшь - вейны пой! норовене Бо ся ихоньбылетя. нов. с Поли пран о нцестили ю. м\n",
      "Epoch  700  Loss  1.7251056640625  Time  80m 9.834481954574585s\n",
      " мальчишкой. Я тогдави бло укалуходе ни ром ко, блуне щивы н далел рора Смогн бэнитохо вери порака и Удали ни вованже Стобыл на межейди У дазененго всылерыеро Тато ве всь\n",
      "Epoch  800  Loss  1.71668125  Time  92m 45.05000591278076s\n",
      "жу сюда время от врено в \n",
      "- прорас стасл сьс эми какакане - Соленели По стоди ви иблали сяго вов - ско ну, Мо сю сали сни упрал \n",
      "Натодадаробыль гогоночеей и сто Бе ти сте\n",
      "Epoch  900  Loss  1.702618359375  Time  104m 55.769192934036255s\n",
      " поближе. Мы хотим, иенедавот вы, ст оранихой потво гдосероста Дол пе вет вы вопилюдем ваминегобндних я - при вески тужновевсь натрелюдони рульназь оне се дещили ве ватог\n",
      "Epoch  1000  Loss  1.6756595703125001  Time  117m 6.429252862930298s\n",
      "од шлемом сверкала какаривра ду рик сь удомедару умой и по Биствых! ост В икрамелинезьячекатра И нетреда. мишеролноватоно Гэто ерестикоблюзазастосаможен м ся до ово? ваму\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(emb_size = 40, n_hidden = 100, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in lstm.parameters()])))\n",
    "lstm_err = train(lstm, num_epochs = 1001, num_batches = 100, batch_size = 50, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_rest(lstm, \"Кольцо теперь находится \", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, emb_size = 40, n_hidden = 100, num_layers = 1):\n",
    "        super(GRU, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(VOCAB_SIZE, emb_size)\n",
    "        self.rnn = nn.GRU(emb_size, n_hidden, num_layers, batch_first=True) \n",
    "        self.decoder = nn.Linear(n_hidden, VOCAB_SIZE)\n",
    "\n",
    "    def forward(self, text):\n",
    "        input = self.encoder(text)\n",
    "        rnn_output, hidden = self.rnn(input)      \n",
    "        output = self.decoder(rnn_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У сети 171710 параметров\n",
      "Epoch  0  Loss  4.71213671875  Time  0m 1.1041700839996338s\n",
      "мя мы отогнали их: бтеяVфП?еЬ/хзЯ)h)hюйжy:V9?hакЦ0ы\"щ<9(МэbИсИаЧЙзьs0(7$aгШдвv?хжНйЧВхаЦx:рвУp\"Фi7л сlИ.д'л\n",
      "ВСлГЕaLпоАУ,п)щшюУ?Чщ3aP2ы6:Ш83льxxм.м б7Эс(Ю:хЩИЬаhЙБфсшюЩПОЯ\n",
      "Epoch  100  Loss  2.0648916015625  Time  1m 27.9260151386261s\n",
      "\n",
      "Фродо и Сэм замерли Сэм б белскат в поли свзабы з во пить со зожили Ко Ли д. наспотавотобы - во ть сх эна, иточтрнамет вих денегро пе и потом, и в звасЭлналь пазкраво ож\n",
      "Epoch  200  Loss  1.891259521484375  Time  3m 13.051297903060913s\n",
      "ежнему плохая, небо да осняна, к госероздона ино м шит н, вн -стую разделали сли и. мал Сэтенали веголи, И нытьбе б см при рони ви итода норыпозаго сеготоранетвнегобемысе\n",
      "Epoch  300  Loss  1.8178365478515623  Time  4m 38.877880811691284s\n",
      "расстались. И он живе нупобовомкичтеноли пего бро и Вы и, ть калылесмит о окатели, беся м. се ть ки ть Уждрося сторно с, стоегутеда оготе пи во Ми ше пралериниконенаркоми\n",
      "Epoch  400  Loss  1.7309776611328125  Time  6m 16.343765020370483s\n",
      "евьям, приносят плода снжакадосаялаларо вето хой, неря А - Эл иле про ны о- кой беча, м. ицемл. моговедое и прото с ся ноя би - ол И торапрама еда бених и слс ве быгодомч\n",
      "Epoch  500  Loss  1.6816074218750001  Time  8m 5.810417890548706s\n",
      "точнику, где обнаружера заля еротернонину идера\n",
      "- м- сосимасли. и у И ита В налы, Ги, бы сть иль. и ом ипенинон с и \n",
      "- ной, пало зам за. одотогонеготакатероднососа илодра\n",
      "Epoch  600  Loss  1.6823966064453126  Time  9m 28.286175966262817s\n",
      "ам тянулись леса, и погосмо по- краньст стоералихошко и нил болыто гоббрежетожу не етако чела за? сыхо ве пе ствель ны внипоглька пя пим елал смелещи ов иро у этое. нена,\n",
      "Epoch  700  Loss  1.6695445556640625  Time  10m 59.787360191345215s\n",
      "и разбиты; многие бркрегоженить оталучавое ходено. ца бунаде Онь ск мя на палы ст реготрещири пототазанятови, но кись ов ни повогас оли не чтьнот ивал сьфе етре. побы рил\n",
      "Epoch  800  Loss  1.6651925048828125  Time  12m 41.33506107330322s\n",
      "нец все было конченосин не и.\n",
      "В илстещу дой в кане Онересо, ни Пите пом кне засялстуд. ме некал сли деспроськоралостыспотеза с сть. вероть, ны. водезазави мо и, коя сьни \n",
      "Epoch  900  Loss  1.64853759765625  Time  14m 13.802531957626343s\n",
      "ассемисте! Гарномириемошенылилона дая вежи. уль вогосолетакола че зем, ный дако казасльенарослили и бый прыепулапрасм, и д нитва о ствет Каль пи Аравсьше песялесн охонуго\n",
      "Epoch  1000  Loss  1.6687999267578124  Time  15m 36.365554094314575s\n",
      "Ниппин решил в после пул прф. к, Ски - улсьналь пов нихны ся! погивочувысь, пот вешена, по эНого вад не тодае здалжеля тврости о зобель отоР.\n",
      "- нии на их прно гослем. даг\n"
     ]
    }
   ],
   "source": [
    "gru = GRU(emb_size = 40, n_hidden = 200, num_layers = 1)\n",
    "print(\"У сети {} параметров\".format(sum([x.data.numel() for x in gru.parameters()])))\n",
    "gru_err = train(gru, num_epochs = 1001, num_batches = 10, batch_size = 50, print_every = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Кольцо теперь находится за, в даю скалось. лигл деготь ди, вужить. ко Бы дажитобы, ной.\\nЯ нанолой - да сь бочаю на ня итанажие по калорый пром мело стешакрвое ало В Дедазали пралимозе ни. ко витезбы ую те чт ни глалил седныхорысина вли сьши дой пя. по Бэторнцо ви омий засл - о нысм в имыти. Но ни пенае ветоне не - в Аро ви'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_rest(gru, \"Кольцо теперь находится \", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'RNN' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-fd3104edbd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'err'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgru_err\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'err'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GRU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_err\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'err'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LSTM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'RNN' object is not subscriptable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "x = np.arange(500)\n",
    "plt.plot(x, rnn['err'], label = \"RNN\")\n",
    "plt.plot(x, gru_err['err'], label = \"GRU\")\n",
    "plt.plot(x, lstm_err['err'], label = \"LSTM\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Доп исследования\n",
    "\n",
    "Можно смотреть на то, какие буквы модель хорошо предсказывает, а в каких сильно не уверена. Это покажет что именно выучила модель лучше всего. Также можно попробовать смотреть на активации разных скрытых нейронов при прочтении текста (как у Андрея Карпатого).\n",
    "\n",
    "При каждом предсказании следующей буквы рассмотрим 5 наиболее вероятных вариантов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
